{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSJRg60MjucXFX7PkSYRkw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishalbharti29/advance-nlp-assignments/blob/main/21310_vishalbharti_Advance_nlpassignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PWpfrdpJAa6",
        "outputId": "ac68cb5a-b366-4801-c70b-8f533a082a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Imports and Setup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import re\n",
        "import urllib.request  # for downloading if needed\n",
        "\n",
        "print(\"Setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Download Dataset (AG News for text classification, 4 classes)\n",
        "# Note: Using AG News dataset as it matches the class labels in the results (4 classes: World, Sports, Business, Sci/Tech).\n",
        "# If the actual dataset is different, replace the URLs with the correct ones from the assignment repo.\n",
        "\n",
        "train_url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv'\n",
        "test_url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv'\n",
        "\n",
        "def download_csv(url, filename):\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "download_csv(train_url, 'train.csv')\n",
        "download_csv(test_url, 'test.csv')\n",
        "\n",
        "print(\"Dataset downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1hsoUieJLi0",
        "outputId": "e7b9099c-ecf5-4424-9a25-00c734c651d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load and Pre-process Data (Custom Pre-processing)\n",
        "def load_data(filename):\n",
        "    df = pd.read_csv(filename, header=None, names=['label', 'title', 'text'])\n",
        "    df['full_text'] = df['title'] + ' ' + df['text']\n",
        "    texts = df['full_text'].values\n",
        "    labels = df['label'].values - 1  # 0-3 for classes\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = load_data('train.csv')\n",
        "test_texts, test_labels = load_data('test.csv')\n",
        "\n",
        "# Split train into train/val\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}\")\n",
        "\n",
        "# Custom Pre-processing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # remove punctuation\n",
        "    return text.split()\n",
        "\n",
        "# Build vocabulary (limit to 5000 most common words for speed)\n",
        "from collections import Counter\n",
        "\n",
        "all_words = []\n",
        "for text in train_texts:\n",
        "    all_words.extend(preprocess_text(text))\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "vocab = ['<UNK>', '<PAD>'] + [word for word, _ in word_counts.most_common(5000)]\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "\n",
        "# Convert to indices and pad\n",
        "max_len = 50  # Reduced for speed\n",
        "\n",
        "def text_to_indices(text, word_to_idx, max_len):\n",
        "    tokens = preprocess_text(text)\n",
        "    indices = [word_to_idx.get(word, 0) for word in tokens]  # 0 for UNK\n",
        "    if len(indices) < max_len:\n",
        "        indices += [1] * (max_len - len(indices))  # 1 for PAD\n",
        "    return indices[:max_len]\n",
        "\n",
        "X_train = np.array([text_to_indices(text, word_to_idx, max_len) for text in train_texts])\n",
        "X_val = np.array([text_to_indices(text, word_to_idx, max_len) for text in val_texts])\n",
        "X_test = np.array([text_to_indices(text, word_to_idx, max_len) for text in test_texts])\n",
        "\n",
        "y_train = train_labels\n",
        "y_val = val_labels\n",
        "y_test = test_labels\n",
        "\n",
        "print(\"Data prepared.\")\n",
        "print(f\"X_train shape: {X_train.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQGpd8J1JOH0",
        "outputId": "66481988-4844-42d3-f67e-e435b2bc5f3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 96000, Val: 24000, Test: 7600\n",
            "Vocab size: 5002\n",
            "Data prepared.\n",
            "X_train shape: (96000, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Utility Functions\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    return -np.mean(np.log(y_pred[range(len(y_true)), y_true] + 1e-8))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-np.clip(z, -250, 250)))\n",
        "\n",
        "class Adam:\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.t = 0\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "\n",
        "    def update(self, param_name, param, grad):\n",
        "        if param_name not in self.m:\n",
        "            self.m[param_name] = np.zeros_like(param)\n",
        "            self.v[param_name] = np.zeros_like(param)\n",
        "        self.t += 1\n",
        "        m = self.m[param_name]\n",
        "        v = self.v[param_name]\n",
        "        m = self.beta1 * m + (1 - self.beta1) * grad\n",
        "        v = self.beta2 * v + (1 - self.beta2) * (grad ** 2)\n",
        "        m_hat = m / (1 - self.beta1 ** self.t)\n",
        "        v_hat = v / (1 - self.beta2 ** self.t)\n",
        "        param -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "        self.m[param_name] = m\n",
        "        self.v[param_name] = v\n",
        "        return param"
      ],
      "metadata": {
        "id": "nUoDDWbBJQbS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Embedding Layer\n",
        "class Embedding:\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.W = np.random.normal(0, 0.1, (vocab_size, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "        embeds = np.zeros((batch_size, seq_len, self.embed_dim))\n",
        "        for b in range(batch_size):\n",
        "            for t in range(seq_len):\n",
        "                embeds[b, t] = self.W[x[b, t]]\n",
        "        return embeds"
      ],
      "metadata": {
        "id": "lXBYcQc9Jz7k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Optimized Simple RNN Model with Adam and Embedding Grad\n",
        "class SimpleRNN:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.001):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.Wxh = np.random.normal(0, 0.01, (input_dim, hidden_dim))\n",
        "        self.Whh = np.random.normal(0, 0.01, (hidden_dim, hidden_dim))\n",
        "        self.bh = np.zeros((1, hidden_dim))\n",
        "        self.Why = np.random.normal(0, 0.01, (hidden_dim, output_dim))\n",
        "        self.by = np.zeros((1, output_dim))\n",
        "        self.optimizer = Adam(lr=lr)\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        if h0 is None:\n",
        "            h0 = np.zeros((batch_size, self.hidden_dim))\n",
        "        hs = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
        "        hs[:, 0] = h0\n",
        "        for t in range(seq_len):\n",
        "            hs[:, t+1] = np.tanh(np.dot(x[:, t], self.Wxh) + np.dot(hs[:, t], self.Whh) + self.bh)\n",
        "        y = np.dot(hs[:, -1], self.Why) + self.by\n",
        "        p = softmax(y)\n",
        "        return p, hs\n",
        "\n",
        "    def backward(self, x, hs, dy):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        # Output layer\n",
        "        dWhy = np.dot(hs[:, -1].T, dy) / batch_size\n",
        "        dby = np.sum(dy, axis=0, keepdims=True) / batch_size\n",
        "        dh = np.dot(dy, self.Why.T)\n",
        "\n",
        "        # BPTT\n",
        "        dWxh = np.zeros_like(self.Wxh)\n",
        "        dWhh = np.zeros_like(self.Whh)\n",
        "        dbh = np.zeros_like(self.bh)\n",
        "        d_embeds = np.zeros_like(x)\n",
        "        dh_next = dh\n",
        "        for t in reversed(range(seq_len)):\n",
        "            dh_raw = dh_next * (1 - hs[:, t+1] ** 2)\n",
        "            dWxh += np.dot(x[:, t].T, dh_raw) / batch_size\n",
        "            dWhh += np.dot(hs[:, t].T, dh_raw) / batch_size\n",
        "            dbh += np.sum(dh_raw, axis=0, keepdims=True) / batch_size\n",
        "            dh_next = np.dot(dh_raw, self.Whh.T)\n",
        "            d_embeds[:, t] += np.dot(dh_raw, self.Wxh.T) / batch_size  # Average grad\n",
        "\n",
        "        # Update params with Adam\n",
        "        self.Wxh = self.optimizer.update('Wxh', self.Wxh, dWxh)\n",
        "        self.Whh = self.optimizer.update('Whh', self.Whh, dWhh)\n",
        "        self.bh = self.optimizer.update('bh', self.bh, dbh)\n",
        "        self.Why = self.optimizer.update('Why', self.Why, dWhy)\n",
        "        self.by = self.optimizer.update('by', self.by, dby)\n",
        "\n",
        "        return d_embeds\n",
        "\n",
        "class RNNClassifier:\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, lr=0.001):\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = SimpleRNN(embed_dim, hidden_dim, num_classes, lr)\n",
        "        self.optimizer = Adam(lr=lr)  # For embedding\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding.forward(x)\n",
        "        p, hs = self.rnn.forward(embeds)\n",
        "        return p\n",
        "\n",
        "    def train_step(self, x, y):\n",
        "        batch_size = x.shape[0]\n",
        "        p = self.forward(x)\n",
        "        dy = p.copy()\n",
        "        dy[range(batch_size), y] -= 1\n",
        "        embeds = self.embedding.forward(x)\n",
        "        _, hs = self.rnn.forward(embeds)\n",
        "        d_embeds = self.rnn.backward(embeds, hs, dy)\n",
        "        # Update embedding\n",
        "        grad_W = np.zeros_like(self.embedding.W)\n",
        "        for b in range(batch_size):\n",
        "            for t in range(max_len):\n",
        "                if x[b, t] != 1:  # Skip PAD\n",
        "                    grad_W[x[b, t]] += d_embeds[b, t]\n",
        "        grad_W /= batch_size\n",
        "        self.embedding.W = self.optimizer.update('embed_W', self.embedding.W, grad_W)\n",
        "        loss = cross_entropy_loss(y, p)\n",
        "        return loss\n",
        "\n",
        "    def save_weights(self, prefix):\n",
        "        np.save(f'{prefix}_embed.npy', self.embedding.W)\n",
        "        np.save(f'{prefix}_Wxh.npy', self.rnn.Wxh)\n",
        "        np.save(f'{prefix}_Whh.npy', self.rnn.Whh)\n",
        "        np.save(f'{prefix}_bh.npy', self.rnn.bh)\n",
        "        np.save(f'{prefix}_Why.npy', self.rnn.Why)\n",
        "        np.save(f'{prefix}_by.npy', self.rnn.by)\n",
        "\n",
        "    def load_weights(self, prefix):\n",
        "        self.embedding.W = np.load(f'{prefix}_embed.npy')\n",
        "        self.rnn.Wxh = np.load(f'{prefix}_Wxh.npy')\n",
        "        self.rnn.Whh = np.load(f'{prefix}_Whh.npy')\n",
        "        self.rnn.bh = np.load(f'{prefix}_bh.npy')\n",
        "        self.rnn.Why = np.load(f'{prefix}_Why.npy')\n",
        "        self.rnn.by = np.load(f'{prefix}_by.npy')"
      ],
      "metadata": {
        "id": "DLhGXVpbJ05R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Optimized Simple LSTM Model with Adam and Embedding Grad\n",
        "class SimpleLSTM:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.001):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.Wf = np.random.normal(0, 0.01, (input_dim, hidden_dim))\n",
        "        self.Wi = np.random.normal(0, 0.01, (input_dim, hidden_dim))\n",
        "        self.Wo = np.random.normal(0, 0.01, (input_dim, hidden_dim))\n",
        "        self.Wg = np.random.normal(0, 0.01, (input_dim, hidden_dim))\n",
        "        self.Uf = np.random.normal(0, 0.01, (hidden_dim, hidden_dim))\n",
        "        self.Ui = np.random.normal(0, 0.01, (hidden_dim, hidden_dim))\n",
        "        self.Uo = np.random.normal(0, 0.01, (hidden_dim, hidden_dim))\n",
        "        self.Ug = np.random.normal(0, 0.01, (hidden_dim, hidden_dim))\n",
        "        self.bf = np.zeros((1, hidden_dim))\n",
        "        self.bi = np.zeros((1, hidden_dim))\n",
        "        self.bo = np.zeros((1, hidden_dim))\n",
        "        self.bg = np.zeros((1, hidden_dim))\n",
        "        self.Why = np.random.normal(0, 0.01, (hidden_dim, output_dim))\n",
        "        self.by = np.zeros((1, output_dim))\n",
        "        self.optimizer = Adam(lr=lr)\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        if h0 is None:\n",
        "            h0 = np.zeros((batch_size, self.hidden_dim))\n",
        "        if c0 is None:\n",
        "            c0 = np.zeros((batch_size, self.hidden_dim))\n",
        "        hs = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
        "        cs = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
        "        hs[:, 0] = h0\n",
        "        cs[:, 0] = c0\n",
        "        fs, is_, os_, gs = [], [], [], []\n",
        "        for t in range(seq_len):\n",
        "            f = sigmoid(np.dot(x[:, t], self.Wf) + np.dot(hs[:, t], self.Uf) + self.bf)\n",
        "            i = sigmoid(np.dot(x[:, t], self.Wi) + np.dot(hs[:, t], self.Ui) + self.bi)\n",
        "            o = sigmoid(np.dot(x[:, t], self.Wo) + np.dot(hs[:, t], self.Uo) + self.bo)\n",
        "            g = np.tanh(np.dot(x[:, t], self.Wg) + np.dot(hs[:, t], self.Ug) + self.bg)\n",
        "            cs[:, t+1] = f * cs[:, t] + i * g\n",
        "            hs[:, t+1] = o * np.tanh(cs[:, t+1])\n",
        "            fs.append(f)\n",
        "            is_.append(i)\n",
        "            os_.append(o)\n",
        "            gs.append(g)\n",
        "        y = np.dot(hs[:, -1], self.Why) + self.by\n",
        "        p = softmax(y)\n",
        "        return p, hs, cs, fs, is_, os_, gs\n",
        "\n",
        "    def backward(self, x, hs, cs, fs, is_, os_, gs, dy):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        # Output layer\n",
        "        dWhy = np.dot(hs[:, -1].T, dy) / batch_size\n",
        "        dby = np.sum(dy, axis=0, keepdims=True) / batch_size\n",
        "        dh = np.dot(dy, self.Why.T)\n",
        "        dc_next = np.zeros_like(dh)\n",
        "        dh_next = np.zeros_like(dh)\n",
        "\n",
        "        dWf = np.zeros_like(self.Wf)\n",
        "        dWi = np.zeros_like(self.Wi)\n",
        "        dWo = np.zeros_like(self.Wo)\n",
        "        dWg = np.zeros_like(self.Wg)\n",
        "        dUf = np.zeros_like(self.Uf)\n",
        "        dUi = np.zeros_like(self.Ui)\n",
        "        dUo = np.zeros_like(self.Uo)\n",
        "        dUg = np.zeros_like(self.Ug)\n",
        "        dbf = np.zeros_like(self.bf)\n",
        "        dbi = np.zeros_like(self.bi)\n",
        "        dbo = np.zeros_like(self.bo)\n",
        "        dbg = np.zeros_like(self.bg)\n",
        "        d_embeds = np.zeros_like(x)\n",
        "\n",
        "        for t in reversed(range(seq_len)):\n",
        "            o = os_[t]\n",
        "            c = cs[:, t+1]\n",
        "            g = gs[t]\n",
        "            i = is_[t]\n",
        "            f = fs[t]\n",
        "            h_prev = hs[:, t]\n",
        "            c_prev = cs[:, t]\n",
        "\n",
        "            dc = (dh * o * (1 - np.tanh(c)**2)) + dc_next\n",
        "            do = dh * np.tanh(c) * o * (1 - o)\n",
        "            dg = dc * i * (1 - g**2)\n",
        "            di = dc * g * i * (1 - i)\n",
        "            df = dc * c_prev * f * (1 - f)\n",
        "\n",
        "            dh_prev = np.dot(do, self.Uo.T) + np.dot(dg, self.Ug.T) + np.dot(di, self.Ui.T) + np.dot(df, self.Uf.T) + dh_next\n",
        "            dc_prev = dc * f\n",
        "\n",
        "            dx_t = np.dot(do, self.Wo.T) + np.dot(dg, self.Wg.T) + np.dot(di, self.Wi.T) + np.dot(df, self.Wf.T)\n",
        "            d_embeds[:, t] += dx_t / batch_size\n",
        "\n",
        "            # Gate grads\n",
        "            dWo += np.dot(x[:, t].T, do) / batch_size\n",
        "            dWi += np.dot(x[:, t].T, di) / batch_size\n",
        "            dWf += np.dot(x[:, t].T, df) / batch_size\n",
        "            dWg += np.dot(x[:, t].T, dg) / batch_size\n",
        "            dUo += np.dot(h_prev.T, do) / batch_size\n",
        "            dUi += np.dot(h_prev.T, di) / batch_size\n",
        "            dUf += np.dot(h_prev.T, df) / batch_size\n",
        "            dUg += np.dot(h_prev.T, dg) / batch_size\n",
        "            dbo += np.sum(do, axis=0, keepdims=True) / batch_size\n",
        "            dbi += np.sum(di, axis=0, keepdims=True) / batch_size\n",
        "            dbf += np.sum(df, axis=0, keepdims=True) / batch_size\n",
        "            dbg += np.sum(dg, axis=0, keepdims=True) / batch_size\n",
        "\n",
        "            dh_next = dh_prev\n",
        "            dc_next = dc_prev\n",
        "\n",
        "        # Update with Adam\n",
        "        self.Wf = self.optimizer.update('Wf', self.Wf, dWf)\n",
        "        self.Wi = self.optimizer.update('Wi', self.Wi, dWi)\n",
        "        self.Wo = self.optimizer.update('Wo', self.Wo, dWo)\n",
        "        self.Wg = self.optimizer.update('Wg', self.Wg, dWg)\n",
        "        self.Uf = self.optimizer.update('Uf', self.Uf, dUf)\n",
        "        self.Ui = self.optimizer.update('Ui', self.Ui, dUi)\n",
        "        self.Uo = self.optimizer.update('Uo', self.Uo, dUo)\n",
        "        self.Ug = self.optimizer.update('Ug', self.Ug, dUg)\n",
        "        self.bf = self.optimizer.update('bf', self.bf, dbf)\n",
        "        self.bi = self.optimizer.update('bi', self.bi, dbi)\n",
        "        self.bo = self.optimizer.update('bo', self.bo, dbo)\n",
        "        self.bg = self.optimizer.update('bg', self.bg, dbg)\n",
        "        self.Why = self.optimizer.update('Why', self.Why, dWhy)\n",
        "        self.by = self.optimizer.update('by', self.by, dby)\n",
        "\n",
        "        return d_embeds\n",
        "\n",
        "class LSTMClassifier:\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, lr=0.001):\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = SimpleLSTM(embed_dim, hidden_dim, num_classes, lr)\n",
        "        self.optimizer = Adam(lr=lr)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding.forward(x)\n",
        "        p, _, _, _, _, _, _ = self.lstm.forward(embeds)\n",
        "        return p\n",
        "\n",
        "    def train_step(self, x, y):\n",
        "        batch_size = x.shape[0]\n",
        "        p = self.forward(x)\n",
        "        dy = p.copy()\n",
        "        dy[range(batch_size), y] -= 1\n",
        "        embeds = self.embedding.forward(x)\n",
        "        _, hs, cs, fs, is_, os_, gs = self.lstm.forward(embeds)\n",
        "        d_embeds = self.lstm.backward(embeds, hs, cs, fs, is_, os_, gs, dy)\n",
        "        # Update embedding\n",
        "        grad_W = np.zeros_like(self.embedding.W)\n",
        "        for b in range(batch_size):\n",
        "            for t in range(max_len):\n",
        "                if x[b, t] != 1:  # Skip PAD\n",
        "                    grad_W[x[b, t]] += d_embeds[b, t]\n",
        "        grad_W /= batch_size\n",
        "        self.embedding.W = self.optimizer.update('embed_W', self.embedding.W, grad_W)\n",
        "        loss = cross_entropy_loss(y, p)\n",
        "        return loss\n",
        "\n",
        "    def save_weights(self, prefix):\n",
        "        np.save(f'{prefix}_embed.npy', self.embedding.W)\n",
        "        np.save(f'{prefix}_Wf.npy', self.lstm.Wf)\n",
        "        np.save(f'{prefix}_Wi.npy', self.lstm.Wi)\n",
        "        np.save(f'{prefix}_Wo.npy', self.lstm.Wo)\n",
        "        np.save(f'{prefix}_Wg.npy', self.lstm.Wg)\n",
        "        np.save(f'{prefix}_Uf.npy', self.lstm.Uf)\n",
        "        np.save(f'{prefix}_Ui.npy', self.lstm.Ui)\n",
        "        np.save(f'{prefix}_Uo.npy', self.lstm.Uo)\n",
        "        np.save(f'{prefix}_Ug.npy', self.lstm.Ug)\n",
        "        np.save(f'{prefix}_bf.npy', self.lstm.bf)\n",
        "        np.save(f'{prefix}_bi.npy', self.lstm.bi)\n",
        "        np.save(f'{prefix}_bo.npy', self.lstm.bo)\n",
        "        np.save(f'{prefix}_bg.npy', self.lstm.bg)\n",
        "        np.save(f'{prefix}_Why.npy', self.lstm.Why)\n",
        "        np.save(f'{prefix}_by.npy', self.lstm.by)\n",
        "\n",
        "    def load_weights(self, prefix):\n",
        "        self.embedding.W = np.load(f'{prefix}_embed.npy')\n",
        "        self.lstm.Wf = np.load(f'{prefix}_Wf.npy')\n",
        "        self.lstm.Wi = np.load(f'{prefix}_Wi.npy')\n",
        "        self.lstm.Wo = np.load(f'{prefix}_Wo.npy')\n",
        "        self.lstm.Wg = np.load(f'{prefix}_Wg.npy')\n",
        "        self.lstm.Uf = np.load(f'{prefix}_Uf.npy')\n",
        "        self.lstm.Ui = np.load(f'{prefix}_Ui.npy')\n",
        "        self.lstm.Uo = np.load(f'{prefix}_Uo.npy')\n",
        "        self.lstm.Ug = np.load(f'{prefix}_Ug.npy')\n",
        "        self.lstm.bf = np.load(f'{prefix}_bf.npy')\n",
        "        self.lstm.bi = np.load(f'{prefix}_bi.npy')\n",
        "        self.lstm.bo = np.load(f'{prefix}_bo.npy')\n",
        "        self.lstm.bg = np.load(f'{prefix}_bg.npy')\n",
        "        self.lstm.Why = np.load(f'{prefix}_Why.npy')\n",
        "        self.lstm.by = np.load(f'{prefix}_by.npy')"
      ],
      "metadata": {
        "id": "HkqiA52fJ3GT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Optimized Simple Transformer Model with Adam and Embedding Grad\n",
        "class SimpleTransformer:\n",
        "    def __init__(self, embed_dim, head_dim, num_classes, max_len=50, lr=0.001):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.head_dim = head_dim\n",
        "        self.Wq = np.random.normal(0, 0.01, (embed_dim, head_dim))\n",
        "        self.Wk = np.random.normal(0, 0.01, (embed_dim, head_dim))\n",
        "        self.Wv = np.random.normal(0, 0.01, (embed_dim, head_dim))\n",
        "        self.Wo = np.random.normal(0, 0.01, (head_dim, embed_dim))\n",
        "        self.Wff = np.random.normal(0, 0.01, (embed_dim, embed_dim))\n",
        "        self.bff = np.zeros((1, embed_dim))\n",
        "        self.Why = np.random.normal(0, 0.01, (embed_dim, num_classes))\n",
        "        self.by = np.zeros((1, num_classes))\n",
        "        self.optimizer = Adam(lr=lr)\n",
        "        self.pos_enc = self._positional_encoding(max_len, embed_dim)\n",
        "\n",
        "    def _positional_encoding(self, max_len, d_model):\n",
        "        pe = np.zeros((max_len, d_model))\n",
        "        position = np.arange(0, max_len).reshape(max_len, 1)\n",
        "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = np.sin(position * div_term)\n",
        "        pe[:, 1::2] = np.cos(position * div_term)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, embeds):\n",
        "        batch_size, seq_len, _ = embeds.shape\n",
        "        pos = self.pos_enc[:seq_len]\n",
        "        pos = np.repeat(pos[np.newaxis, :, :], batch_size, axis=0)\n",
        "        x = embeds + pos\n",
        "\n",
        "        Q = np.matmul(x, self.Wq)\n",
        "        K = np.matmul(x, self.Wk)\n",
        "        V = np.matmul(x, self.Wv)\n",
        "        scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(self.head_dim)\n",
        "        attn = softmax(scores)\n",
        "        attn_h = np.matmul(attn, V)\n",
        "        attn_out = np.matmul(attn_h, self.Wo)\n",
        "        ff = np.tanh(np.matmul(attn_out, self.Wff) + self.bff)\n",
        "        ff_mean = np.mean(ff, axis=1)\n",
        "        y = np.matmul(ff_mean, self.Why) + self.by\n",
        "        p = softmax(y)\n",
        "        return p, x, Q, K, V, attn, attn_h, attn_out, ff, ff_mean\n",
        "\n",
        "    def backward(self, embeds, x, Q, K, V, attn, attn_h, attn_out, ff, ff_mean, dy):\n",
        "        batch_size, seq_len, _ = embeds.shape\n",
        "        # Output layer\n",
        "        d_ff_mean = np.dot(dy, self.Why.T)\n",
        "        dWhy = np.dot(ff_mean.T, dy) / batch_size\n",
        "        dby = np.sum(dy, axis=0, keepdims=True) / batch_size\n",
        "\n",
        "        d_ff = np.repeat(d_ff_mean[:, np.newaxis, :], seq_len, axis=1) / seq_len\n",
        "        d_lin = d_ff * (1 - ff ** 2)\n",
        "        d_attn_out = np.dot(d_lin, self.Wff.T)\n",
        "        dWff = np.dot(attn_out.transpose(0,2,1).reshape(-1, self.embed_dim).T, d_lin.reshape(-1, self.embed_dim)) / batch_size\n",
        "        dbff = np.sum(d_lin, axis=(0,1)) / batch_size\n",
        "\n",
        "        d_attn_h = np.dot(d_attn_out, self.Wo.T)\n",
        "        dWo = np.dot(attn_h.transpose(0,2,1).reshape(-1, self.head_dim).T, d_attn_out.reshape(-1, self.embed_dim)) / batch_size\n",
        "\n",
        "        dV = np.matmul(attn.transpose(0,2,1), d_attn_h)\n",
        "        d_attn = np.matmul(d_attn_h, V.transpose(0,2,1))\n",
        "\n",
        "        d_scores = attn * d_attn - attn * np.sum(attn * d_attn, axis=-1)[:,:,np.newaxis]\n",
        "        d_scores /= np.sqrt(self.head_dim)\n",
        "\n",
        "        dQ = np.matmul(d_scores, K)\n",
        "        dK = np.matmul(d_scores.transpose(0,2,1), Q)\n",
        "        dV += np.matmul(attn.transpose(0,2,1), d_attn_h)  # Already have dV\n",
        "\n",
        "        dx = np.dot(dQ, self.Wq.T) + np.dot(dK, self.Wk.T) + np.dot(dV, self.Wv.T)\n",
        "        dWq = np.dot(x.transpose(0,2,1).reshape(-1, self.embed_dim).T, dQ.reshape(-1, self.head_dim)) / batch_size\n",
        "        dWk = np.dot(x.transpose(0,2,1).reshape(-1, self.embed_dim).T, dK.reshape(-1, self.head_dim)) / batch_size\n",
        "        dWv = np.dot(x.transpose(0,2,1).reshape(-1, self.embed_dim).T, dV.reshape(-1, self.head_dim)) / batch_size\n",
        "\n",
        "        d_embeds = dx  # Since pos has no grad\n",
        "\n",
        "        # Update with Adam\n",
        "        self.Wq = self.optimizer.update('Wq', self.Wq, dWq)\n",
        "        self.Wk = self.optimizer.update('Wk', self.Wk, dWk)\n",
        "        self.Wv = self.optimizer.update('Wv', self.Wv, dWv)\n",
        "        self.Wo = self.optimizer.update('Wo', self.Wo, dWo)\n",
        "        self.Wff = self.optimizer.update('Wff', self.Wff, dWff)\n",
        "        self.bff = self.optimizer.update('bff', self.bff, dbff)\n",
        "        self.Why = self.optimizer.update('Why', self.Why, dWhy)\n",
        "        self.by = self.optimizer.update('by', self.by, dby)\n",
        "\n",
        "        return d_embeds / batch_size  # Average\n",
        "\n",
        "class TransformerClassifier:\n",
        "    def __init__(self, vocab_size, embed_dim, head_dim, num_classes, lr=0.001):\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.transformer = SimpleTransformer(embed_dim, head_dim, num_classes, max_len, lr)\n",
        "        self.optimizer = Adam(lr=lr)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding.forward(x)\n",
        "        p, _, _, _, _, _, _, _, _, _ = self.transformer.forward(embeds)\n",
        "        return p\n",
        "\n",
        "    def train_step(self, x, y):\n",
        "        batch_size = x.shape[0]\n",
        "        p = self.forward(x)\n",
        "        dy = p.copy()\n",
        "        dy[range(batch_size), y] -= 1\n",
        "        embeds = self.embedding.forward(x)\n",
        "        _, x_trans, Q, K, V, attn, attn_h, attn_out, ff, ff_mean = self.transformer.forward(embeds)\n",
        "        d_embeds = self.transformer.backward(embeds, x_trans, Q, K, V, attn, attn_h, attn_out, ff, ff_mean, dy)\n",
        "        # Update embedding\n",
        "        grad_W = np.zeros_like(self.embedding.W)\n",
        "        for b in range(batch_size):\n",
        "            for t in range(max_len):\n",
        "                if x[b, t] != 1:\n",
        "                    grad_W[x[b, t]] += d_embeds[b, t]\n",
        "        grad_W /= batch_size\n",
        "        self.embedding.W = self.optimizer.update('embed_W', self.embedding.W, grad_W)\n",
        "        loss = cross_entropy_loss(y, p)\n",
        "        return loss\n",
        "\n",
        "    def save_weights(self, prefix):\n",
        "        np.save(f'{prefix}_embed.npy', self.embedding.W)\n",
        "        np.save(f'{prefix}_Wq.npy', self.transformer.Wq)\n",
        "        np.save(f'{prefix}_Wk.npy', self.transformer.Wk)\n",
        "        np.save(f'{prefix}_Wv.npy', self.transformer.Wv)\n",
        "        np.save(f'{prefix}_Wo.npy', self.transformer.Wo)\n",
        "        np.save(f'{prefix}_Wff.npy', self.transformer.Wff)\n",
        "        np.save(f'{prefix}_bff.npy', self.transformer.bff)\n",
        "        np.save(f'{prefix}_Why.npy', self.transformer.Why)\n",
        "        np.save(f'{prefix}_by.npy', self.transformer.by)\n",
        "\n",
        "    def load_weights(self, prefix):\n",
        "        self.embedding.W = np.load(f'{prefix}_embed.npy')\n",
        "        self.transformer.Wq = np.load(f'{prefix}_Wq.npy')\n",
        "        self.transformer.Wk = np.load(f'{prefix}_Wk.npy')\n",
        "        self.transformer.Wv = np.load(f'{prefix}_Wv.npy')\n",
        "        self.transformer.Wo = np.load(f'{prefix}_Wo.npy')\n",
        "        self.transformer.Wff = np.load(f'{prefix}_Wff.npy')\n",
        "        self.transformer.bff = np.load(f'{prefix}_bff.npy')\n",
        "        self.transformer.Why = np.load(f'{prefix}_Why.npy')\n",
        "        self.transformer.by = np.load(f'{prefix}_by.npy')"
      ],
      "metadata": {
        "id": "lwSH8MLnJ6Vs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Training Function (Commented out for submission)\n",
        "# To train, uncomment and run. Removed subsample for better performance. May take time (hours on CPU for full data).\n",
        "# Recommend running on a machine with good CPU or reduce data size for testing.\n",
        "# For faster, set subsample_size = 10000 or so.\n",
        "\n",
        "def train_model(model, X, y, epochs=10, batch_size=64, val_X=None, val_y=None):\n",
        "    # No subsample for better performance\n",
        "    # For test, uncomment: indices = np.random.choice(len(X), 10000, replace=False); X = X[indices]; y = y[indices]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        perm = np.random.permutation(len(X))\n",
        "        X = X[perm]\n",
        "        y = y[perm]\n",
        "\n",
        "        total_loss = 0\n",
        "        num_batches = len(X) // batch_size\n",
        "        for i in range(0, len(X), batch_size):\n",
        "            batch_X = X[i:i+batch_size]\n",
        "            batch_y = y[i:i+batch_size]\n",
        "            if len(batch_X) < batch_size:\n",
        "                continue  # Skip small batch\n",
        "            loss = model.train_step(batch_X, batch_y)\n",
        "            total_loss += loss\n",
        "            if (i // batch_size) % 100 == 0:\n",
        "                print(f\"Batch {i // batch_size}/{num_batches}, Loss: {loss}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Avg Loss: {total_loss / num_batches}\")\n",
        "\n",
        "        if val_X is not None:\n",
        "            val_p = model.forward(val_X)\n",
        "            val_loss = cross_entropy_loss(val_y, val_p)\n",
        "            val_f1 = f1_score(val_y, np.argmax(val_p, axis=1), average='macro')\n",
        "            print(f\"Val Loss: {val_loss}, Val Macro F1: {val_f1}\")\n",
        "\n",
        "# Example training (commented)\n",
        "embed_dim = 100\n",
        "hidden_dim = 64  # Reduced for speed\n",
        "num_classes = 4\n",
        "head_dim = 64\n",
        "lr = 0.001\n",
        "\n",
        "# rnn_model = RNNClassifier(vocab_size, embed_dim, hidden_dim, num_classes, lr)\n",
        "# train_model(rnn_model, X_train, y_train, epochs=10, val_X=X_val, val_y=y_val)\n",
        "# rnn_model.save_weights('rnn')\n",
        "\n",
        "# lstm_model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_classes, lr)\n",
        "# train_model(lstm_model, X_train, y_train, epochs=10, val_X=X_val, val_y=y_val)\n",
        "# lstm_model.save_weights('lstm')\n",
        "\n",
        "# trans_model = TransformerClassifier(vocab_size, embed_dim, head_dim, num_classes, lr)\n",
        "# train_model(trans_model, X_train, y_train, epochs=10, val_X=X_val, val_y=y_val)\n",
        "# trans_model.save_weights('transformer')\n",
        "\n",
        "print(\"Training code ready (commented out).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5YdGR6bJ-BG",
        "outputId": "92d3e767-5bf3-4b98-cf69-60ce89180be1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training code ready (commented out).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Load Weights from GitHub Repo (Inference Only)\n",
        "# Upload the saved .npy files to your repo after training.\n",
        "# Adjust the download lines for each model.\n",
        "\n",
        "repo_base = \"https://raw.githubusercontent.com/Vishalbharti29/advance-nlp-assignments/main/\"\n",
        "\n",
        "def download_weight(url, filename):\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# Example for RNN\n",
        "# download_weight(repo_base + 'rnn_embed.npy', 'rnn_embed.npy')\n",
        "# ... for all params\n",
        "\n",
        "embed_dim = 100\n",
        "hidden_dim = 64\n",
        "num_classes = 4\n",
        "head_dim = 64\n",
        "lr = 0.001  # Not used for load\n",
        "\n",
        "rnn_model = RNNClassifier(vocab_size, embed_dim, hidden_dim, num_classes, lr)\n",
        "# rnn_model.load_weights('rnn')  # Load local or downloaded\n",
        "\n",
        "lstm_model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_classes, lr)\n",
        "# lstm_model.load_weights('lstm')\n",
        "\n",
        "trans_model = TransformerClassifier(vocab_size, embed_dim, head_dim, num_classes, lr)\n",
        "# trans_model.load_weights('transformer')\n",
        "\n",
        "print(\"Models loaded for inference.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbh8w6YpKBzS",
        "outputId": "87c1063f-ef18-4e10-bb85-b2f4fe817d74"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models loaded for inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Inference and Classification Reports\n",
        "rnn_pred = np.argmax(rnn_model.forward(X_test), axis=1)\n",
        "rnn_f1 = f1_score(y_test, rnn_pred, average='macro')\n",
        "print(f\"RNN Macro F1: {rnn_f1}\")\n",
        "print(classification_report(y_test, rnn_pred, target_names=['World', 'Sports', 'Business', 'Sci/Tech']))\n",
        "\n",
        "lstm_pred = np.argmax(lstm_model.forward(X_test), axis=1)\n",
        "lstm_f1 = f1_score(y_test, lstm_pred, average='macro')\n",
        "print(f\"LSTM Macro F1: {lstm_f1}\")\n",
        "print(classification_report(y_test, lstm_pred, target_names=['World', 'Sports', 'Business', 'Sci/Tech']))\n",
        "\n",
        "trans_pred = np.argmax(trans_model.forward(X_test), axis=1)\n",
        "trans_f1 = f1_score(y_test, trans_pred, average='macro')\n",
        "print(f\"Transformer Macro F1: {trans_f1}\")\n",
        "print(classification_report(y_test, trans_pred, target_names=['World', 'Sports', 'Business', 'Sci/Tech']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMaarVqzKEZ8",
        "outputId": "39ae298b-5a05-4363-8770-5fa9faf14065"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN Macro F1: 0.1237024427272822\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.25      0.95      0.39      1900\n",
            "      Sports       0.26      0.02      0.03      1900\n",
            "    Business       0.15      0.01      0.02      1900\n",
            "    Sci/Tech       0.39      0.03      0.06      1900\n",
            "\n",
            "    accuracy                           0.25      7600\n",
            "   macro avg       0.26      0.25      0.12      7600\n",
            "weighted avg       0.26      0.25      0.12      7600\n",
            "\n",
            "LSTM Macro F1: 0.12466119570825829\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.36      0.02      0.04      1900\n",
            "      Sports       0.28      0.02      0.03      1900\n",
            "    Business       0.19      0.02      0.04      1900\n",
            "    Sci/Tech       0.25      0.94      0.39      1900\n",
            "\n",
            "    accuracy                           0.25      7600\n",
            "   macro avg       0.27      0.25      0.12      7600\n",
            "weighted avg       0.27      0.25      0.12      7600\n",
            "\n",
            "Transformer Macro F1: 0.1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.00      0.00      0.00      1900\n",
            "      Sports       0.25      1.00      0.40      1900\n",
            "    Business       0.00      0.00      0.00      1900\n",
            "    Sci/Tech       0.00      0.00      0.00      1900\n",
            "\n",
            "    accuracy                           0.25      7600\n",
            "   macro avg       0.06      0.25      0.10      7600\n",
            "weighted avg       0.06      0.25      0.10      7600\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}